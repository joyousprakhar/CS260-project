{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "AUXmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hBDeenH7Yfy"
      },
      "source": [
        "##Creating the data folders\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6bt3l957X6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7591a535-7a37-4a09-d269-081f08100069"
      },
      "source": [
        "!mkdir data/\n",
        "!mkdir data/cifar10\n",
        "!mkdir data/cifar100\n",
        "!mkdir data/mnist\n",
        "!mkdir data/fashionmnist\n",
        "!mkdir data/svhn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data/’: File exists\n",
            "mkdir: cannot create directory ‘data/cifar10’: File exists\n",
            "mkdir: cannot create directory ‘data/cifar100’: File exists\n",
            "mkdir: cannot create directory ‘data/mnist’: File exists\n",
            "mkdir: cannot create directory ‘data/fashionmnist’: File exists\n",
            "mkdir: cannot create directory ‘data/svhn’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3dKwL7A7ZuM"
      },
      "source": [
        "## Upload the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bh_82Gq7Z8S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZkIuhLE2S_Z"
      },
      "source": [
        "## Code for data_load.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujTeGnKRk8bJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12042ba3-3721-4fd1-ec2b-90da9c45a50f"
      },
      "source": [
        "%%writefile data_load.py\n",
        "\n",
        "import numpy as np\n",
        "import torch.utils.data as Data\n",
        "from PIL import Image\n",
        "import tools\n",
        "import torch\n",
        "from random import choice\n",
        "import random \n",
        "\n",
        "class mnist_dataset(Data.Dataset):\n",
        "    def __init__(self, train=True, transform=None, target_transform=None, dataset='mnist', noise_type='symmetric', noise_rate=0.5, split_per=0.9, random_seed=1, num_class=10):\n",
        "            \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train \n",
        "        original_images = np.load('data/mnist/train_images.npy')\n",
        "        original_labels = np.load('data/mnist/train_labels.npy')\n",
        "\n",
        "        # clean images and noisy labels (training and validation)\n",
        "        self.train_data, self.val_data, self.train_labels, self.val_labels = tools.dataset_split(original_images, \n",
        "                                                                             original_labels, dataset, noise_type, noise_rate, split_per, random_seed, num_class)\n",
        "\n",
        "       \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "           \n",
        "        if self.train:\n",
        "            img, label = self.train_data[index], self.train_labels[index]\n",
        "        else:\n",
        "            img, label = self.val_data[index], self.val_labels[index]\n",
        "            \n",
        "        img = Image.fromarray(img)\n",
        "           \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "     \n",
        "        return img, label, index\n",
        "    def __len__(self):\n",
        "            \n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "   \n",
        "        else:\n",
        "            return len(self.val_data)\n",
        " \n",
        "class mnist_test_dataset(Data.Dataset):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "            \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        \n",
        "        self.test_data = np.load('data/mnist/test_images.npy')\n",
        "        self.test_labels = np.load('data/mnist/test_labels.npy') - 1 # 0-9\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        img, label = self.test_data[index], self.test_labels[index]\n",
        "        \n",
        "        img = Image.fromarray(img)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "     \n",
        "        return img, label, index\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.test_data)\n",
        "  \n",
        "    \n",
        "class cifar10_dataset(Data.Dataset):\n",
        "    def __init__(self, train=True, transform=None, target_transform=None, dataset='cifar10', noise_type='symmetric', noise_rate=0.5, split_per=0.9, random_seed=1, num_class=10):\n",
        "            \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train \n",
        "        \n",
        "        original_images = np.load('data/cifar10/train_images.npy')\n",
        "        original_labels = np.load('data/cifar10/train_labels.npy')\n",
        "        \n",
        "\n",
        "        # clean images and noisy labels (training and validation)\n",
        "        self.train_data, self.val_data, self.train_labels, self.val_labels = tools.dataset_split(original_images, \n",
        "                                                                             original_labels, dataset, noise_type, noise_rate, split_per, random_seed, num_class)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        if self.train:      \n",
        "            self.train_data = self.train_data.reshape((-1, 3, 32, 32))\n",
        "            self.train_data = self.train_data.transpose((0, 2, 3, 1))\n",
        "        \n",
        "        else:\n",
        "            self.val_data = self.val_data.reshape((-1, 3, 32, 32))\n",
        "            self.val_data = self.val_data.transpose((0, 2, 3, 1))\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "           \n",
        "        if self.train:\n",
        "            img, label = self.train_data[index], self.train_labels[index]\n",
        "            \n",
        "        else:\n",
        "            img, label = self.val_data[index], self.val_labels[index]\n",
        "            \n",
        "        img = Image.fromarray(img)\n",
        "           \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "     \n",
        "        return img, label, index\n",
        "    def __len__(self):\n",
        "            \n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        \n",
        "        else:\n",
        "            return len(self.val_data)\n",
        "        \n",
        "class cifar10_test_dataset(Data.Dataset):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "            \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "           \n",
        "        self.test_data = np.load('data/cifar10/test_images.npy')\n",
        "        self.test_labels = np.load('data/cifar10/test_labels.npy')\n",
        "        self.test_data = self.test_data.reshape((-1, 3, 32, 32))\n",
        "        self.test_data = self.test_data.transpose((0, 2, 3, 1)) \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        img, label = self.test_data[index], self.test_labels[index]\n",
        "        \n",
        "        img = Image.fromarray(img)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "     \n",
        "        return img, label, index\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.test_data)\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "class cifar100_dataset(Data.Dataset):\n",
        "    def __init__(self, train=True, transform=None, target_transform=None, dataset='cifar100', noise_type='symmetric', noise_rate=0.5, split_per=0.9, random_seed=1, num_class=100):\n",
        "            \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train \n",
        "        \n",
        "        original_images = np.load('data/cifar100/train_images.npy')\n",
        "        original_labels = np.load('data/cifar100/train_labels.npy')\n",
        "\n",
        "        # clean images and noisy labels (training and validation)\n",
        "        self.train_data, self.val_data, self.train_labels, self.val_labels = tools.dataset_split(original_images, \n",
        "                                                                             original_labels, dataset, noise_type, noise_rate, split_per, random_seed, num_class)\n",
        "\n",
        "\n",
        "\n",
        "        if self.train:      \n",
        "            self.train_data = self.train_data.reshape((-1, 3, 32, 32))\n",
        "            self.train_data = self.train_data.transpose((0, 2, 3, 1)) \n",
        "        \n",
        "        else:\n",
        "            self.val_data = self.val_data.reshape((-1, 3, 32, 32))\n",
        "            self.val_data = self.val_data.transpose((0, 2, 3, 1))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "           \n",
        "        if self.train:\n",
        "            img, label = self.train_data[index], self.train_labels[index]\n",
        "            \n",
        "        else:\n",
        "            img, label = self.val_data[index], self.val_labels[index]\n",
        "            \n",
        "        img = Image.fromarray(img)\n",
        "           \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "     \n",
        "        return img, label, index\n",
        "    def __len__(self):\n",
        "            \n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        \n",
        "        else:\n",
        "            return len(self.val_data)\n",
        "        \n",
        "        \n",
        "class cifar100_test_dataset(Data.Dataset):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "            \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "           \n",
        "        self.test_data = np.load('data/cifar100/test_images.npy')\n",
        "        self.test_labels = np.load('data/cifar100/test_labels.npy')\n",
        "        self.test_data = self.test_data.reshape((-1, 3, 32, 32))\n",
        "        self.test_data = self.test_data.transpose((0, 2, 3, 1)) \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        img, label = self.test_data[index], self.test_labels[index]\n",
        "        \n",
        "        img = Image.fromarray(img)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "     \n",
        "        return img, label, index\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.test_data)\n",
        "\n",
        "          \n",
        "class fmnist_dataset(Data.Dataset):\n",
        "    def __init__(self, train=True, transform=None, target_transform=None, dataset='fmnist', noise_type='symmetric', noise_rate=0.5, split_per=0.9, random_seed=1, num_class=10):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train\n",
        "        original_images = np.load('data/fashionmnist/train_images.npy').reshape((-1, 1, 28, 28))\n",
        "        original_labels = np.load('data/fashionmnist/train_labels.npy')\n",
        "    \n",
        "\n",
        "        self.train_data, self.val_data, self.train_labels, self.val_labels = tools.dataset_split(original_images, \n",
        "                                                                             original_labels, dataset, noise_type, noise_rate, split_per, random_seed, num_class)\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.train:\n",
        "            img, label = self.train_data[index], self.train_labels[index]\n",
        "        else:\n",
        "            img, label = self.val_data[index], self.val_labels[index]\n",
        "        img = torch.from_numpy(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return img, label, index\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "\n",
        "        else:\n",
        "            return len(self.val_data)\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "class fmnist_test_dataset(Data.Dataset):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.test_data = np.load('data/fashionmnist/test_images.npy').reshape((-1, 1, 28, 28))\n",
        "        self.test_labels = np.load('data/fashionmnist/test_labels.npy')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.test_data[index], self.test_labels[index]\n",
        "        img = torch.from_numpy(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return img, label, index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_load.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgPjvLvY2Z_s"
      },
      "source": [
        "## Code for tools.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9Oz1MEk8bK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476790bd-3e97-44b4-8953-9f534c795224"
      },
      "source": [
        "%%writefile tools.py\n",
        "\n",
        "import numpy as np\n",
        "import utils\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from math import inf\n",
        "from scipy import stats\n",
        "from torchvision.transforms import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "def transition_matrix_generate(noise_rate=0.5, num_classes=10):\n",
        "    P = np.ones((num_classes, num_classes))\n",
        "    n = noise_rate\n",
        "    P = (n / (num_classes - 1)) * P\n",
        "\n",
        "    if n > 0.0:\n",
        "        # 0 -> 1\n",
        "        P[0, 0] = 1. - n\n",
        "        for i in range(1, num_classes-1):\n",
        "            P[i, i] = 1. - n\n",
        "        P[num_classes-1, num_classes-1] = 1. - n\n",
        "    return P\n",
        "\n",
        "\n",
        "def fit(X, num_classes, filter_outlier=False):\n",
        "    # number of classes\n",
        "    c = num_classes\n",
        "    T = np.empty((c, c))\n",
        "    eta_corr = X\n",
        "    for i in np.arange(c):\n",
        "        if not filter_outlier:\n",
        "            idx_best = np.argmax(eta_corr[:, i])\n",
        "        else:\n",
        "            eta_thresh = np.percentile(eta_corr[:, i], 97,interpolation='higher')\n",
        "            robust_eta = eta_corr[:, i]\n",
        "            robust_eta[robust_eta >= eta_thresh] = 0.0\n",
        "            idx_best = np.argmax(robust_eta)\n",
        "        for j in np.arange(c):\n",
        "            T[i, j] = eta_corr[idx_best, j]\n",
        "    return T\n",
        "\n",
        "\n",
        "# flip clean labels to noisy labels\n",
        "# train set and val set split\n",
        "def dataset_split(train_images, train_labels, dataset='mnist', noise_type='symmetric', noise_rate=0.5, split_per=0.9, random_seed=1, num_classes=10):\n",
        "    \n",
        "    clean_train_labels = train_labels[:, np.newaxis]\n",
        "    \n",
        "    if noise_type == 'symmetric':\n",
        "         noisy_labels, real_noise_rate, transition_matrix = utils.noisify_multiclass_symmetric(clean_train_labels, \n",
        "                                                                                               noise=noise_rate, \n",
        "                                                                                               random_state=random_seed, \n",
        "                                                                                               nb_classes=num_classes)\n",
        "    if noise_type == 'pairflip':\n",
        "        noisy_labels, real_noise_rate, transition_matrix = utils.noisify_pairflip(clean_train_labels,\n",
        "                                                                                          noise=noise_rate,\n",
        "                                                                                          random_state=random_seed,\n",
        "                                                                                          nb_classes=num_classes)\n",
        "    if noise_type == 'asymmetric' and dataset == 'mnist':\n",
        "        noisy_labels, real_noise_rate, transition_matrix = utils.noisify_multiclass_asymmetric_mnist(clean_train_labels,\n",
        "                                                                                                    noise=noise_rate,\n",
        "                                                                                                    random_state=random_seed,\n",
        "                                                                                                    nb_classes=num_classes)\n",
        "        \n",
        "    if noise_type == 'asymmetric' and dataset == 'fmnist':\n",
        "        noisy_labels, real_noise_rate, transition_matrix = utils.noisify_multiclass_asymmetric_fashionmnist(clean_train_labels,\n",
        "                                                                                                    noise=noise_rate,\n",
        "                                                                                                    random_state=random_seed,\n",
        "                                                                                                    nb_classes=num_classes)\n",
        "    \n",
        "    if noise_type == 'asymmetric' and dataset == 'cifar10':\n",
        "        noisy_labels, real_noise_rate, transition_matrix = utils.noisify_multiclass_asymmetric_cifar10(clean_train_labels,\n",
        "                                                                                                      noise=noise_rate,\n",
        "                                                                                                      random_state=random_seed,\n",
        "                                                                                                      nb_classes=num_classes)\n",
        "        \n",
        "    if noise_type == 'asymmetric' and dataset == 'cifar100':\n",
        "        noisy_labels, real_noise_rate, transition_matrix = utils.noisify_multiclass_asymmetric_cifar100(clean_train_labels,\n",
        "                                                                                                       noise=noise_rate,\n",
        "                                                                                                       random_state=random_seed,\n",
        "                                                                                                       nb_classes=num_classes)\n",
        "        \n",
        "    \n",
        "        \n",
        "    if noise_type == 'instance' and dataset == 'mnist':\n",
        "        data = torch.from_numpy(train_images).float()\n",
        "        targets = torch.from_numpy(train_labels)\n",
        "        dataset_ = zip(data, targets)\n",
        "        noisy_labels = get_instance_noisy_label(n=noise_rate, dataset=dataset_, labels=targets, num_classes=10, feature_size=784, norm_std=0.1, seed=random_seed)\n",
        "        \n",
        "        \n",
        "    if noise_type == 'instance' and dataset == 'fmnist':\n",
        "        data = torch.from_numpy(train_images).float()\n",
        "        targets = torch.from_numpy(train_labels)\n",
        "        dataset_ = zip(data, targets)\n",
        "        noisy_labels = get_instance_noisy_label(n=noise_rate, dataset=dataset_, labels=targets, num_classes=10, feature_size=784, norm_std=0.1, seed=random_seed)\n",
        "        \n",
        "    \n",
        "    if noise_type == 'instance' and dataset == 'cifar10':\n",
        "        data = torch.from_numpy(train_images).float()\n",
        "        targets = torch.from_numpy(train_labels)\n",
        "        dataset_ = zip(data, targets)\n",
        "        noisy_labels = get_instance_noisy_label(n=noise_rate, dataset=dataset_, labels=targets, num_classes=10, feature_size=3072, norm_std=0.1, seed=random_seed)\n",
        "        \n",
        "    if noise_type == 'instance' and dataset == 'cifar100':\n",
        "        data = torch.from_numpy(train_images).float()\n",
        "        targets = torch.from_numpy(train_labels)\n",
        "        dataset_ = zip(data, targets)\n",
        "        noisy_labels = get_instance_noisy_label(n=noise_rate, dataset=dataset_, labels=targets, num_classes=100, feature_size=3072, norm_std=0.1, seed=random_seed)\n",
        "\n",
        "    \n",
        "\n",
        "    noisy_labels = noisy_labels.squeeze()\n",
        "    num_samples = int(noisy_labels.shape[0])\n",
        "    np.random.seed(random_seed)\n",
        "    train_set_index = np.random.choice(num_samples, int(num_samples*split_per), replace=False)\n",
        "    index = np.arange(train_images.shape[0])\n",
        "    val_set_index = np.delete(index, train_set_index)\n",
        "\n",
        "    train_set, val_set = train_images[train_set_index, :], train_images[val_set_index, :]\n",
        "    train_labels, val_labels = noisy_labels[train_set_index], noisy_labels[val_set_index]\n",
        "\n",
        "    return train_set, val_set, train_labels, val_labels\n",
        "\n",
        "def dataset_split_without_noise(train_images, train_labels, noise_rate, split_per=0.9, random_seed=1, num_class=196):\n",
        "    total_labels = train_labels[:, np.newaxis]\n",
        "    #    print(noisy_labels)\n",
        "    num_samples = int(total_labels.shape[0])\n",
        "    np.random.seed(random_seed)\n",
        "    train_set_index = np.random.choice(num_samples, int(num_samples * split_per), replace=False)\n",
        "    index = np.arange(train_images.shape[0])\n",
        "    val_set_index = np.delete(index, train_set_index)\n",
        "    train_set, val_set = train_images[train_set_index], train_images[val_set_index]\n",
        "    train_labels, val_labels = total_labels[train_set_index], total_labels[val_set_index]\n",
        "\n",
        "    return train_set, val_set, train_labels.squeeze(), val_labels.squeeze()\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets, _ in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    print(mean)\n",
        "    print(std)\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def get_instance_noisy_label(n, dataset, labels, num_classes, feature_size, norm_std, seed):\n",
        "    # n -> noise_rate\n",
        "    # dataset -> mnist, cifar10, cifar100 # not train_loader\n",
        "    # labels -> labels (targets)\n",
        "    # label_num -> class number\n",
        "    # feature_size -> the size of input images (e.g. 28*28)\n",
        "    # norm_std -> default 0.1\n",
        "    # seed -> random_seed\n",
        "\n",
        "    label_num = num_classes\n",
        "    np.random.seed(int(seed))\n",
        "    torch.manual_seed(int(seed))\n",
        "    torch.cuda.manual_seed(int(seed))\n",
        "\n",
        "    P = []\n",
        "    flip_distribution = stats.truncnorm((0 - n) / norm_std, (1 - n) / norm_std, loc=n, scale=norm_std)\n",
        "    # flip_distribution = stats.beta(a=0.01, b=(0.01 / n) - 0.01, loc=0, scale=1)\n",
        "    flip_rate = flip_distribution.rvs(labels.shape[0])\n",
        "\n",
        "    if isinstance(labels, list):\n",
        "        labels = torch.FloatTensor(labels)\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    W = np.random.randn(label_num, feature_size, label_num)\n",
        "\n",
        "    W = torch.FloatTensor(W).cuda()\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        # 1*m *  m*10 = 1*10\n",
        "        x = x.cuda()\n",
        "        A = x.view(1, -1).mm(W[y]).squeeze(0)\n",
        "        # print(A.shape)\n",
        "        A[y] = -inf\n",
        "        A = flip_rate[i] * F.softmax(A, dim=0)\n",
        "        A[y] += 1 - flip_rate[i]\n",
        "        P.append(A)\n",
        "    P = torch.stack(P, 0).cpu().numpy()\n",
        "    #np.save(\"transition_matrix.npy\", P)\n",
        "\n",
        "    l = [i for i in range(label_num)]\n",
        "    new_label = [np.random.choice(l, p=P[i]) for i in range(labels.shape[0])]\n",
        "    # print(f'noise rate = {(new_label != np.array(labels.cpu())).mean()}')\n",
        "\n",
        "    record = [[0 for _ in range(label_num)] for i in range(label_num)]\n",
        "\n",
        "    for a, b in zip(labels, new_label):\n",
        "        a, b = int(a), int(b)\n",
        "        record[a][b] += 1\n",
        "        #\n",
        "    print('****************************************')\n",
        "    print('following is flip percentage:')\n",
        "\n",
        "    for i in range(label_num):\n",
        "        sum_i = sum(record[i])\n",
        "        for j in range(label_num):\n",
        "            if i != j:\n",
        "                print(f\"{record[i][j] / sum_i: .2f}\", end='\\t')\n",
        "            else:\n",
        "                print(f\"{record[i][j] / sum_i: .2f}\", end='\\t')\n",
        "        # print()\n",
        "\n",
        "    pidx = np.random.choice(range(P.shape[0]), 1000)\n",
        "    cnt = 0\n",
        "    for i in range(1000):\n",
        "        if labels[pidx[i]] == 0:\n",
        "            a = P[pidx[i], :]\n",
        "            for j in range(label_num):\n",
        "                print(f\"{a[j]:.2f}\", end=\"\\t\")\n",
        "            print()\n",
        "            cnt += 1\n",
        "        if cnt >= 10:\n",
        "            break\n",
        "    #print(P)\n",
        "    return np.array(new_label)\n",
        "\n",
        "\n",
        "def transform_target(label):\n",
        "    label = np.array(label)\n",
        "    target = torch.from_numpy(label).long()\n",
        "    return target  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tools.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzcx3v3n2dXi"
      },
      "source": [
        "## Code for utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UYoM1Wk2d0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7965fda-5c60-4505-b5eb-c3c34fa365b3"
      },
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import copy\n",
        "import hashlib\n",
        "import errno\n",
        "import numpy as np\n",
        "from numpy.testing import assert_array_almost_equal\n",
        "\n",
        "def check_integrity(fpath, md5):\n",
        "    if not os.path.isfile(fpath):\n",
        "        return False\n",
        "    md5o = hashlib.md5()\n",
        "    with open(fpath, 'rb') as f:\n",
        "        # read in 1MB chunks\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
        "            md5o.update(chunk)\n",
        "    md5c = md5o.hexdigest()\n",
        "    if md5c != md5:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def download_url(url, root, filename, md5):\n",
        "    from six.moves import urllib\n",
        "\n",
        "    root = os.path.expanduser(root)\n",
        "    fpath = os.path.join(root, filename)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(root)\n",
        "    except OSError as e:\n",
        "        if e.errno == errno.EEXIST:\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    # downloads file\n",
        "    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n",
        "        print('Using downloaded and verified file: ' + fpath)\n",
        "    else:\n",
        "        try:\n",
        "            print('Downloading ' + url + ' to ' + fpath)\n",
        "            urllib.request.urlretrieve(url, fpath)\n",
        "        except:\n",
        "            if url[:5] == 'https':\n",
        "                url = url.replace('https:', 'http:')\n",
        "                print('Failed download. Trying https -> http instead.'\n",
        "                      ' Downloading ' + url + ' to ' + fpath)\n",
        "                urllib.request.urlretrieve(url, fpath)\n",
        "\n",
        "\n",
        "def list_dir(root, prefix=False):\n",
        "    \"\"\"List all directories at a given root\n",
        "    Args:\n",
        "        root (str): Path to directory whose folders need to be listed\n",
        "        prefix (bool, optional): If true, prepends the path to each result, otherwise\n",
        "            only returns the name of the directories found\n",
        "    \"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    directories = list(\n",
        "        filter(\n",
        "            lambda p: os.path.isdir(os.path.join(root, p)),\n",
        "            os.listdir(root)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if prefix is True:\n",
        "        directories = [os.path.join(root, d) for d in directories]\n",
        "\n",
        "    return directories\n",
        "\n",
        "\n",
        "def list_files(root, suffix, prefix=False):\n",
        "    \"\"\"List all files ending with a suffix at a given root\n",
        "    Args:\n",
        "        root (str): Path to directory whose folders need to be listed\n",
        "        suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png').\n",
        "            It uses the Python \"str.endswith\" method and is passed directly\n",
        "        prefix (bool, optional): If true, prepends the path to each result, otherwise\n",
        "            only returns the name of the files found\n",
        "    \"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    files = list(\n",
        "        filter(\n",
        "            lambda p: os.path.isfile(os.path.join(root, p)) and p.endswith(suffix),\n",
        "            os.listdir(root)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if prefix is True:\n",
        "        files = [os.path.join(root, d) for d in files]\n",
        "\n",
        "    return files\n",
        "\n",
        "# basic function\n",
        "def multiclass_noisify(y, P, random_state=1):\n",
        "    \"\"\" Flip classes according to transition probability matrix T.\n",
        "    It expects a number between 0 and the number of classes - 1.\n",
        "    \"\"\"\n",
        "#    print (np.max(y), P.shape[0])\n",
        "    assert P.shape[0] == P.shape[1]\n",
        "    assert np.max(y) < P.shape[0]\n",
        "\n",
        "    # row stochastic matrix\n",
        "    assert_array_almost_equal(P.sum(axis=1), np.ones(P.shape[1]))\n",
        "    assert (P >= 0.0).all()\n",
        "\n",
        "    m = y.shape[0]\n",
        "    new_y = y.copy()\n",
        "    flipper = np.random.RandomState(random_state)\n",
        "\n",
        "    for idx in np.arange(m):\n",
        "        i = y[idx]\n",
        "        # draw a vector with only an 1\n",
        "        flipped = flipper.multinomial(1, P[i, :][0], 1)[0]\n",
        "        new_y[idx] = np.where(flipped == 1)[0]\n",
        "\n",
        "    return new_y\n",
        "\n",
        "\n",
        "# noisify_pairflip call the function \"multiclass_noisify\"\n",
        "def noisify_pairflip(y_train, noise, random_state=1, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the pair\n",
        "    \"\"\"\n",
        "    P = np.eye(nb_classes)\n",
        "    n = noise\n",
        "\n",
        "    if n > 0.0:\n",
        "        # 0 -> 1\n",
        "        P[0, 0], P[0, 1] = 1. - n, n\n",
        "        for i in range(1, nb_classes-1):\n",
        "            P[i, i], P[i, i + 1] = 1. - n, n\n",
        "        P[nb_classes-1, nb_classes-1], P[nb_classes-1, 0] = 1. - n, n\n",
        "\n",
        "        y_train_noisy = multiclass_noisify(y_train, P=P,\n",
        "                                           random_state=random_state)\n",
        "        actual_noise = (y_train_noisy != y_train).mean()\n",
        "        assert actual_noise > 0.0\n",
        "        print('Actual noise %.2f' % actual_noise)\n",
        "        y_train = y_train_noisy\n",
        "    # print (P)\n",
        "\n",
        "    return y_train, actual_noise,P\n",
        "\n",
        "def noisify_multiclass_symmetric(y_train, noise, random_state=None, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the symmetric way\n",
        "    \"\"\"\n",
        "    P = np.ones((nb_classes, nb_classes))\n",
        "    n = noise\n",
        "    P = (n / (nb_classes - 1)) * P\n",
        "\n",
        "    if n > 0.0:\n",
        "        # 0 -> 1\n",
        "        P[0, 0] = 1. - n\n",
        "        for i in range(1, nb_classes-1):\n",
        "            P[i, i] = 1. - n\n",
        "        P[nb_classes-1, nb_classes-1] = 1. - n\n",
        "\n",
        "        y_train_noisy = multiclass_noisify(y_train, P=P,\n",
        "                                           random_state=random_state)\n",
        "        actual_noise = (y_train_noisy != y_train).mean()\n",
        "        assert actual_noise > 0.0\n",
        "#        print('Actual noise %.2f' % actual_noise)\n",
        "        y_train = y_train_noisy\n",
        "#    print (P)\n",
        "\n",
        "    return y_train, actual_noise,P\n",
        "\n",
        "def noisify_trid(y_train, noise, random_state=1, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the trid\n",
        "    \"\"\"\n",
        "    P = np.eye(nb_classes)\n",
        "    n = noise\n",
        "\n",
        "    if n > 0.0:\n",
        "        # 0 -> 1\n",
        "        P[0, 0], P[0, 1], P[0, nb_classes-1] = 1. - n, n / 2, n /2\n",
        "        for i in range(1, nb_classes-1):\n",
        "            P[i, i], P[i, i + 1], P[i, i - 1]  = 1. - n, n / 2, n /2\n",
        "        P[nb_classes-1, nb_classes-1], P[nb_classes-1, 0], P[nb_classes-1, nb_classes-2]  = 1. - n, n / 2, n /2\n",
        "\n",
        "        y_train_noisy = multiclass_noisify(y_train, P=P,\n",
        "                                           random_state=random_state)\n",
        "        actual_noise = (y_train_noisy != y_train).mean()\n",
        "        y_train = y_train_noisy\n",
        "    print (P)\n",
        "\n",
        "    return y_train, actual_noise, P\n",
        "\n",
        "\n",
        "\n",
        "def noisify_multiclass_asymmetric_mnist(y_train, noise, random_state=None, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the symmetric way\n",
        "    \"\"\"\n",
        "    P = np.eye(10)\n",
        "    n = noise\n",
        "\n",
        "    # 2 -> 7\n",
        "    P[2, 2], P[2, 7] = 1. - n, n\n",
        "\n",
        "    # 5 <-> 6\n",
        "    P[5, 5], P[5, 6] = 1. - n, n\n",
        "    P[6, 6], P[6, 5] = 1. - n, n\n",
        "\n",
        "    # 3 -> 8\n",
        "    P[3, 3], P[3, 8] = 1. - n, n\n",
        "\n",
        "    y_train_noisy = multiclass_noisify(y_train, P=P, random_state=random_state)\n",
        "    actual_noise = (y_train_noisy != y_train).mean()\n",
        "    assert actual_noise > 0.0\n",
        "    print('Actual noise %.2f' % actual_noise)\n",
        "\n",
        "    y_train = y_train_noisy\n",
        "    # print (P)\n",
        "\n",
        "    return y_train, actual_noise, P\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def noisify_multiclass_asymmetric_fashionmnist(y_train, noise, random_state=None, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the symmetric way\n",
        "    \"\"\"\n",
        "    P = np.eye(10)\n",
        "    n = noise\n",
        "    # 0 -> 6\n",
        "    P[0, 0], P[0, 6] = 1. - n, n\n",
        "    # 2 -> 4\n",
        "    P[2, 2], P[2, 4] = 1. - n, n\n",
        "\n",
        "    # 5 <-> 7\n",
        "    P[5, 5], P[5, 7] = 1. - n, n\n",
        "    P[7, 7], P[7, 5] = 1. - n, n\n",
        "\n",
        "    # 3 -> 8\n",
        "    #P[3, 3], P[3, 8] = 1. - n, n\n",
        "\n",
        "    y_train_noisy = multiclass_noisify(y_train, P=P, random_state=random_state)\n",
        "    actual_noise = (y_train_noisy != y_train).mean()\n",
        "    assert actual_noise > 0.0\n",
        "    print('Actual noise %.2f' % actual_noise)\n",
        "\n",
        "    y_train = y_train_noisy\n",
        "    # print (P)\n",
        "\n",
        "    return y_train, actual_noise, P\n",
        "\n",
        "\n",
        "def build_for_cifar100(size, noise):\n",
        "    \"\"\" random flip between two random classes.\n",
        "    \"\"\"\n",
        "    assert(noise >= 0.) and (noise <= 1.)\n",
        "\n",
        "    P = (1. - noise) * np.eye(size)\n",
        "    for i in np.arange(size - 1):\n",
        "        P[i, i+1] = noise\n",
        "\n",
        "    # adjust last row\n",
        "    P[size-1, 0] = noise\n",
        "\n",
        "    assert_array_almost_equal(P.sum(axis=1), 1, 1)\n",
        "    return P\n",
        "\n",
        "def other_class(n_classes, current_class):\n",
        "    \"\"\"\n",
        "    Returns a list of class indices excluding the class indexed by class_ind\n",
        "    :param nb_classes: number of classes in the task\n",
        "    :param class_ind: the class index to be omitted\n",
        "    :return: one random class that != class_ind\n",
        "    \"\"\"\n",
        "    if current_class < 0 or current_class >= n_classes:\n",
        "        error_str = \"class_ind must be within the range (0, nb_classes - 1)\"\n",
        "        raise ValueError(error_str)\n",
        "\n",
        "    other_class_list = list(range(n_classes))\n",
        "    other_class_list.remove(current_class)\n",
        "    other_class = np.random.choice(other_class_list)\n",
        "    return other_class\n",
        "\n",
        "def noisify_multiclass_asymmetric_cifar10(y_train, noise, random_state=None, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the symmetric way\n",
        "    \"\"\"\n",
        "    source_class = [9, 2, 3, 5, 4]\n",
        "    target_class = [1, 0, 5, 3, 7]\n",
        "    y_train_ = y_train\n",
        "    for s, t in zip(source_class, target_class):\n",
        "        cls_idx = np.where(np.array(y_train) == s)[0]\n",
        "        n_noisy = int(noise * cls_idx.shape[0])\n",
        "        noisy_sample_index = np.random.choice(cls_idx, n_noisy, replace=False)\n",
        "        for idx in noisy_sample_index:\n",
        "            y_train_[idx] = t\n",
        "    return y_train_, source_class, target_class\n",
        "\n",
        "def noisify_multiclass_asymmetric_cifar100(y_train, noise, random_state=None, nb_classes=100):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the symmetric way\n",
        "    \"\"\"\n",
        "    nb_classes = 100\n",
        "    P = np.eye(nb_classes)\n",
        "    n = noise\n",
        "    nb_superclasses = 20\n",
        "    nb_subclasses = 5\n",
        "\n",
        "    if n > 0.0:\n",
        "        for i in np.arange(nb_superclasses):\n",
        "            init, end = i * nb_subclasses, (i + 1) * nb_subclasses\n",
        "            P[init:end, init:end] = build_for_cifar100(nb_subclasses, n)\n",
        "\n",
        "            y_train_noisy = multiclass_noisify(np.array(y_train), P=P, random_state=random_state)\n",
        "            actual_noise = (y_train_noisy != np.array(y_train)).mean()\n",
        "        assert actual_noise > 0.0\n",
        "        print('Actual noise %.2f' % actual_noise)\n",
        "        targets = y_train_noisy\n",
        "    return targets, actual_noise, P\n",
        "\n",
        "import tools\n",
        "\n",
        "\n",
        "def noisify(dataset='mnist', nb_classes=10, train_labels=None, noise_type=None, noise_rate=0, random_state=1):\n",
        "    #if noise_type == 'instance':\n",
        "        #train_noisy_labels, actual_noise_rate = tools.(train_labels, noise_rate, random_state=1, nb_classes=nb_classes)\n",
        "    if noise_type == 'pairflip':\n",
        "        train_noisy_labels, actual_noise_rate = noisify_pairflip(train_labels, noise_rate, random_state=1, nb_classes=nb_classes)\n",
        "    if noise_type == 'symmetric':\n",
        "        train_noisy_labels, actual_noise_rate = noisify_multiclass_symmetric(train_labels, noise_rate, random_state=1, nb_classes=nb_classes)\n",
        "    return train_noisy_labels, actual_noise_rate\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                nn.init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant(m.weight, 1)\n",
        "            nn.init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                nn.init.constant(m.bias, 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LssgOFB2eH4"
      },
      "source": [
        "## Code for resnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6hJJ9Ir2eRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c61cb87-a211-41a0-e2cf-dea1ae5103fd"
      },
      "source": [
        "%%writefile resnet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "bn_momentum = 0.1\n",
        "eps = 0\n",
        "\n",
        "\n",
        "class NTKConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, ntk_init= False, **kwargs):\n",
        "        super().__init__( *args,**kwargs)\n",
        "        self.ntk_init = ntk_init\n",
        "        fan_in = self.kernel_size[0] * self.kernel_size[1] * self.in_channels\n",
        "        self.scaler = 1\n",
        "        if ntk_init:\n",
        "            self.scaler =  math.sqrt(2. / fan_in)  \n",
        "            \n",
        "    def forward(self, x):\n",
        "        return super().forward(x)*self.scaler\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "\n",
        "        super(Linear, self).__init__()\n",
        "        self.w = nn.Parameter(torch.randn(in_features, out_features))\n",
        "         \n",
        "    def forward(self, x):\n",
        "        x = x.mm(self.w)\n",
        "        return x\n",
        "\n",
        "class NTKLinear(nn.Linear):\n",
        "    \n",
        "    def __init__(self, *args, ntk_init= False, **kwargs):\n",
        "        super().__init__( *args,**kwargs)\n",
        "        self.ntk_init = ntk_init\n",
        "        self.scaler = 1\n",
        "        if ntk_init:\n",
        "            self.scaler =  math.sqrt(2. / self.in_features)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        return super().forward(x)*self.scaler\n",
        "    \n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1,padding_mode = 'zeros'):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return NTKConv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1+int(padding_mode == 'circular'), bias=False,padding_mode = padding_mode)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None,padding_mode='zeros'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, input_channel, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.conv1 = nn.Conv2d(input_channel, 64, kernel_size=3, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        \n",
        "        \n",
        "           \n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "        \n",
        "def ResNet18(input_channel, num_classes):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], input_channel, num_classes)\n",
        "    \n",
        "def ResNet34(input_channel, num_classes):\n",
        "    return ResNet(BasicBlock, [3,4,6,3], input_channel, num_classes)\n",
        "\n",
        "def ResNet50(input_channel, num_classes):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], input_channel, num_classes)\n",
        "\n",
        "def ResNet101(input_channel, num_classes):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], input_channel, num_classes)\n",
        "\n",
        "def ResNet152(input_channel, num_classes):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], input_channel, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing resnet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iorjqP0c3IPK"
      },
      "source": [
        "## Code for model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y8leZhW3HlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f142e70-40d7-402f-84a2-5ea9f38fbfc9"
      },
      "source": [
        "%%writefile model.py\n",
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1   = nn.Linear(400, 120)\n",
        "        self.fc2   = nn.Linear(120, 84)\n",
        "        self.fc3   = nn.Linear(84, 10)\n",
        "       \n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "       \n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD0rlI9k3PMg"
      },
      "source": [
        "## Main Code starts from here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fdA3wBy1Dc7"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from model import LeNet\n",
        "\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.models as tv_models\n",
        "import torch.optim as optim\n",
        "import argparse, sys\n",
        "import numpy as np\n",
        "import datetime\n",
        "import data_load\n",
        "import resnet\n",
        "import tools\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BpFRarw5Rlg"
      },
      "source": [
        "## Defining the arguments to the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYk19tan4xh9"
      },
      "source": [
        "argsstring = ['--dataset', 'fmnist', '--noise_type', 'symmetric', '--noise_rate', '0.7', '--seed', '1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-uZ0Q5T4B9D"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--n', type=int, default=0, help=\"No.\")\n",
        "parser.add_argument('--d', type=str, default='output', help=\"description\")\n",
        "parser.add_argument('--p', type=int, default=0, help=\"print\")\n",
        "parser.add_argument('--c', type=int, default=10, help=\"class\")\n",
        "parser.add_argument('--lr', type=float, default=0.01)\n",
        "parser.add_argument('--result_dir', type=str, help='dir to save result txt files', default='output/results_cdr/')\n",
        "parser.add_argument('--noise_rate', type=float, help='overall corruption rate, should be less than 1', default=0.4)\n",
        "parser.add_argument('--noise_type', type=str, help='[pairflip, symmetric, asymmetric]', default='symmetric')\n",
        "parser.add_argument('--num_gradual', type=int, default=10, help='how many epochs for linear drop rate')\n",
        "parser.add_argument('--dataset', type=str, help='mnist, fmnist, cifar10, cifar100', default='cifar10')\n",
        "parser.add_argument('--n_epoch', type=int, default=100)\n",
        "parser.add_argument('--optimizer', type=str, default='SGD')\n",
        "parser.add_argument('--seed', type=int, default=1)\n",
        "parser.add_argument('--print_freq', type=int, default=350)\n",
        "parser.add_argument('--num_workers', type=int, default=4, help='how many subprocesses to use for data loading')\n",
        "parser.add_argument('--model_type', type=str, help='[ce, ours]', default='cdr')\n",
        "parser.add_argument('--fr_type', type=str, help='forget rate type', default='type_1')\n",
        "parser.add_argument('--split_percentage', type=float, help='train and validation', default=0.9)\n",
        "parser.add_argument('--gpu', type=int, help='ind of gpu', default=0)\n",
        "parser.add_argument('--weight_decay', type=float, help='l2', default=1e-3)\n",
        "parser.add_argument('--momentum', type=int, help='momentum', default=0.9)\n",
        "parser.add_argument('--batch_size', type=int, help='batch_size', default=32)\n",
        "parser.add_argument('--train_len', type=int, help='the number of training data', default=54000)\n",
        "args = parser.parse_args(argsstring)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M--_lAWX5HfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c4a1c5-2bc3-43a2-83fd-0147a219b985"
      },
      "source": [
        "\n",
        "print(args)\n",
        "# Seed\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Hyper Parameters\n",
        "learning_rate = args.lr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=32, c=10, d='output', dataset='fmnist', fr_type='type_1', gpu=0, lr=0.01, model_type='cdr', momentum=0.9, n=0, n_epoch=100, noise_rate=0.7, noise_type='symmetric', num_gradual=10, num_workers=4, optimizer='SGD', p=0, print_freq=350, result_dir='output/results_cdr/', seed=1, split_percentage=0.9, train_len=54000, weight_decay=0.001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S730a44J5lqj"
      },
      "source": [
        "###Function for data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "681uc_CX5kQ_"
      },
      "source": [
        "# load dataset\n",
        "def load_data(args):\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    \n",
        "    if args.dataset=='fmnist':\n",
        "        args.channel = 1\n",
        "        args.feature_size = 28 * 28\n",
        "        args.num_classes = 10\n",
        "        args.n_epoch = 100\n",
        "        args.batch_size = 32\n",
        "        args.num_gradual = 10\n",
        "        args.train_len = int(60000 * 0.9)\n",
        "        train_dataset = data_load.fmnist_dataset(True,\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n",
        "                                        target_transform=tools.transform_target,\n",
        "                                        dataset=args.dataset,\n",
        "                                        noise_type=args.noise_type,\n",
        "                                        noise_rate=args.noise_rate,\n",
        "                                        split_per=args.split_percentage,\n",
        "                                        random_seed=args.seed)\n",
        "\n",
        "        val_dataset = data_load.fmnist_dataset(False,\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n",
        "                                        target_transform=tools.transform_target,\n",
        "                                        dataset=args.dataset,\n",
        "                                        noise_type=args.noise_type,\n",
        "                                        noise_rate=args.noise_rate,\n",
        "                                        split_per=args.split_percentage,\n",
        "                                        random_seed=args.seed)\n",
        "\n",
        "\n",
        "        test_dataset = data_load.fmnist_test_dataset(\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n",
        "                                        target_transform=tools.transform_target)\n",
        "    \n",
        "    \n",
        "    if args.dataset=='mnist':\n",
        "        args.channel = 1\n",
        "        args.feature_size = 28 * 28\n",
        "        args.num_classes = 10\n",
        "        args.n_epoch = 100\n",
        "        args.batch_size = 32\n",
        "        args.num_gradual = 10\n",
        "        args.train_len = int(60000 * 0.9)\n",
        "        train_dataset = data_load.mnist_dataset(True,\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n",
        "                                        target_transform=tools.transform_target,\n",
        "                                        dataset=args.dataset,\n",
        "                                        noise_type=args.noise_type,\n",
        "                                        noise_rate=args.noise_rate,\n",
        "                                        split_per=args.split_percentage,\n",
        "                                        random_seed=args.seed)\n",
        "\n",
        "        val_dataset = data_load.mnist_dataset(False,\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n",
        "                                        target_transform=tools.transform_target,\n",
        "                                        dataset=args.dataset,\n",
        "                                        noise_type=args.noise_type,\n",
        "                                        noise_rate=args.noise_rate,\n",
        "                                        split_per=args.split_percentage,\n",
        "                                        random_seed=args.seed)\n",
        "\n",
        "\n",
        "        test_dataset = data_load.mnist_test_dataset(\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n",
        "                                        target_transform=tools.transform_target)\n",
        "        \n",
        "        \n",
        "    \n",
        "    if args.dataset=='cifar10':\n",
        "        args.channel = 3\n",
        "        args.num_classes = 10\n",
        "        args.feature_size = 3 * 32 * 32\n",
        "        args.n_epoch = 200\n",
        "        args.batch_size = 64\n",
        "        args.num_gradual = 20\n",
        "        args.train_len = int(50000 * 0.9)\n",
        "        train_dataset = data_load.cifar10_dataset(True,\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.RandomCrop(32, padding=4),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n",
        "                                        ]),\n",
        "                                        target_transform=tools.transform_target,\n",
        "                                        dataset=args.dataset,\n",
        "                                        noise_type=args.noise_type,\n",
        "                                        noise_rate=args.noise_rate,\n",
        "                                        split_per=args.split_percentage,\n",
        "                                        random_seed=args.seed)\n",
        "\n",
        "        val_dataset = data_load.cifar10_dataset(False,\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n",
        "                                        ]),\n",
        "                                        target_transform=tools.transform_target,\n",
        "                                        dataset=args.dataset,\n",
        "                                        noise_type=args.noise_type,\n",
        "                                        noise_rate=args.noise_rate,\n",
        "                                        split_per=args.split_percentage,\n",
        "                                        random_seed=args.seed)\n",
        "\n",
        "\n",
        "        test_dataset = data_load.cifar10_test_dataset(\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n",
        "                                        ]),\n",
        "                                        target_transform=tools.transform_target)\n",
        "    \n",
        "    \n",
        "    if args.dataset=='cifar100':\n",
        "        args.channel = 3\n",
        "        args.num_classes = 100\n",
        "        args.feature_size = 3 * 32 * 32\n",
        "        args.n_epoch = 200\n",
        "        args.batch_size = 64\n",
        "        args.num_gradual = 20\n",
        "        args.train_len = int(50000 * 0.9)\n",
        "        train_dataset = data_load.cifar100_dataset(True,\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.RandomCrop(32, padding=4),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n",
        "                                        ]),\n",
        "                                        target_transform=tools.transform_target,\n",
        "                                        dataset=args.dataset,\n",
        "                                        noise_type=args.noise_type,\n",
        "                                        noise_rate=args.noise_rate,\n",
        "                                        split_per=args.split_percentage,\n",
        "                                        random_seed=args.seed)\n",
        "\n",
        "        val_dataset = data_load.cifar100_dataset(False,\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n",
        "                                        ]),\n",
        "                                        target_transform=tools.transform_target,\n",
        "                                        dataset=args.dataset,\n",
        "                                        noise_type=args.noise_type,\n",
        "                                        noise_rate=args.noise_rate,\n",
        "                                        split_per=args.split_percentage,\n",
        "                                        random_seed=args.seed)\n",
        "\n",
        "\n",
        "        test_dataset = data_load.cifar100_test_dataset(\n",
        "                                        transform = transforms.Compose([\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n",
        "                                        ]),\n",
        "                                        target_transform=tools.transform_target)\n",
        "        \n",
        "    \n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR4GKYYj5rYH"
      },
      "source": [
        "save_dir = args.result_dir + '/' + args.dataset + '/%s/' % args.model_type\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.system('mkdir -p %s' % save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q4N8ztt5v4r"
      },
      "source": [
        "### Function for accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0Dz6wQW5wQs"
      },
      "source": [
        "def accuracy(logit, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    output = F.softmax(logit, dim=1)\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QHVNxGz5waA"
      },
      "source": [
        "### Function for training for one step (IMP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M733tNaO5uWb"
      },
      "source": [
        "def train_one_step(net, data, label, indexes, optimizer, criterion, nonzero_ratio, clip):\n",
        "    net.train()\n",
        "    pred = net(data)    ### Model outputs f(x; theta)\n",
        "    aux_output = net.aux_lambda * net.aux[indexes]  ### Auxiliary variable output lambda * b_i \n",
        "    pred += aux_output  ### Final model output for training f(x; theta) + lambda * b_i\n",
        "    loss = criterion(pred, label)\n",
        "    loss.backward()\n",
        "    \n",
        "    to_concat_g = []\n",
        "    to_concat_v = []\n",
        "    for name, param in net.named_parameters():\n",
        "        if 'aux' in name:  ### We don't want auxiliary variables to be considered in metric calculation \n",
        "            continue\n",
        "        if param.dim() in [2, 4]:\n",
        "            to_concat_g.append(param.grad.data.view(-1))\n",
        "            to_concat_v.append(param.data.view(-1))\n",
        "    all_g = torch.cat(to_concat_g)\n",
        "    all_v = torch.cat(to_concat_v)\n",
        "    metric = torch.abs(all_g * all_v)\n",
        "    num_params = all_v.size(0)\n",
        "    nz = int(nonzero_ratio * num_params)\n",
        "    top_values, _ = torch.topk(metric, nz)\n",
        "    thresh = top_values[-1]\n",
        "\n",
        "    ## mask/zero the gradient value of those parameters whose value (|w_i * g_i|)are less than threshold \n",
        "    for name, param in net.named_parameters():\n",
        "        if 'aux' in name: ### We don't want to zero the gradient of auxiliary variables\n",
        "            continue\n",
        "        if param.dim() in [2, 4]:\n",
        "            mask = (torch.abs(param.data * param.grad.data) >= thresh).type(torch.cuda.FloatTensor)\n",
        "            mask = mask * clip\n",
        "            param.grad.data = mask * param.grad.data\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    acc = accuracy(pred, label, topk=(1,))\n",
        "\n",
        "    return float(acc[0]), loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1mHCfCe55ma"
      },
      "source": [
        "### Function for train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQDc355c55xB"
      },
      "source": [
        "def train(train_loader, epoch, model1, optimizer1, args):\n",
        "    model1.train()\n",
        "    train_total=0\n",
        "    train_correct=0\n",
        "    clip_narry = np.linspace(1-args.noise_rate, 1, num=args.num_gradual)\n",
        "    clip_narry = clip_narry[::-1]\n",
        "    if epoch < args.num_gradual:\n",
        "        clip = clip_narry[epoch]\n",
        "   \n",
        "    clip = (1 - args.noise_rate)\n",
        "    for i, (data, labels, indexes) in enumerate(train_loader):\n",
        "        ind=indexes.cpu().numpy().transpose()\n",
        "        data = data.cuda()\n",
        "        labels = labels.cuda()\n",
        "        # Forward + Backward + Optimize\n",
        "        logits1 = model1(data)  ### Model outputs f(x; theta)\n",
        "        aux_output = model1.aux_lambda * model1.aux[indexes]    ### Auxiliary variable output lambda * b_i \n",
        "        logits1 += aux_output   ### Final model output for training f(x; theta) + lambda * b_i\n",
        "        prec1,  = accuracy(logits1, labels, topk=(1, ))\n",
        "        train_total += 1\n",
        "        train_correct += prec1\n",
        "        # Loss transfer \n",
        "\n",
        "        ### Backward pass\n",
        "        prec1, loss = train_one_step(model1, data, labels, indexes, optimizer1, nn.CrossEntropyLoss(), clip, clip)\n",
        "       \n",
        "        if (i+1) % args.print_freq == 0:\n",
        "            print('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Loss1: %.4f' \n",
        "                  %(epoch+1, args.n_epoch, i+1, args.train_len//args.batch_size, prec1, loss.item()))\n",
        "        \n",
        "      \n",
        "    train_acc1=float(train_correct)/float(train_total)\n",
        "    return train_acc1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz-Sk_4f559T"
      },
      "source": [
        "### Function for eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m58Pe59h56FY"
      },
      "source": [
        "# Evaluate the Model\n",
        "def evaluate(test_loader, model1):\n",
        "    \n",
        "    model1.eval()  # Change model to 'eval' mode.\n",
        "    correct1 = 0\n",
        "    total1 = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels, _ in test_loader:\n",
        "            data = data.cuda()\n",
        "            logits1 = model1(data)\n",
        "            outputs1 = F.softmax(logits1, dim=1)\n",
        "            _, pred1 = torch.max(outputs1.data, 1)\n",
        "            total1 += labels.size(0)\n",
        "            correct1 += (pred1.cpu() == labels.long()).sum()\n",
        "\n",
        "        acc1 = 100 * float(correct1) / float(total1)\n",
        "\n",
        "    return acc1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqMhln86C4E"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHODtTWP6DFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fec739-4125-4f73-81b1-a2153895637b"
      },
      "source": [
        "# Data Loader (Input Pipeline)\n",
        "model_str = args.dataset + '_%s_' % args.model_type + args.noise_type + '_' + str(args.noise_rate) + '_' + str(args.seed)\n",
        "txtfile = save_dir + \"/\" + model_str + \".txt\"\n",
        "nowTime = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "if os.path.exists(txtfile):\n",
        "    os.system('mv %s %s' % (txtfile, txtfile + \".bak-%s\" % nowTime))\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "print('loading dataset...')\n",
        "train_dataset, val_dataset, test_dataset = load_data(args)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_size=args.batch_size,\n",
        "                                            num_workers=args.num_workers,\n",
        "                                            drop_last=False,\n",
        "                                            shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                            batch_size=args.batch_size,\n",
        "                                            num_workers=args.num_workers,\n",
        "                                            drop_last=False,\n",
        "                                            shuffle=False)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                            batch_size=args.batch_size,\n",
        "                                            num_workers=args.num_workers,\n",
        "                                            drop_last=False,\n",
        "                                            shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyrlMFGX6DLr"
      },
      "source": [
        "## Transfering data to cuda & model initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dezjv_Op6EcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffcf37f-782d-4a0f-82cb-3e8850765642"
      },
      "source": [
        "# Define models\n",
        "print('building model...')\n",
        "\n",
        "if args.dataset == 'mnist':\n",
        "    clf1 = LeNet()\n",
        "    optimizer1 = torch.optim.SGD(clf1.parameters(), lr=learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n",
        "    scheduler1 = MultiStepLR(optimizer1, milestones=[10, 20], gamma=0.1)\n",
        "elif args.dataset == 'fmnist':\n",
        "    clf1 = resnet.ResNet50(input_channel=1, num_classes=10)\n",
        "    optimizer1 = torch.optim.SGD(clf1.parameters(), lr=learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n",
        "    scheduler1 = MultiStepLR(optimizer1, milestones=[10, 20], gamma=0.1)\n",
        "elif args.dataset == 'cifar10':\n",
        "    clf1 = resnet.ResNet50(input_channel=3, num_classes=10)\n",
        "    optimizer1 = torch.optim.SGD(clf1.parameters(), lr=learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n",
        "    scheduler1 = MultiStepLR(optimizer1, milestones=[40, 80], gamma=0.1)\n",
        "elif args.dataset == 'cifar100':\n",
        "    clf1 = resnet.ResNet50(input_channel=3, num_classes=100)\n",
        "    optimizer1 = torch.optim.SGD(clf1.parameters(), lr=learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n",
        "    scheduler1 = MultiStepLR(optimizer1, milestones=[40, 80], gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iin09k-mXn9A"
      },
      "source": [
        "## Creating auxiliary variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rywgJ3nwXoGu"
      },
      "source": [
        "my_params = []\n",
        "for name, param in clf1.named_parameters():\n",
        "    my_params.append(param)\n",
        "\n",
        "param_group = {'params': my_params, 'weight_decay': args.weight_decay, 'lr' : args.lr, 'momentum' : args.momentum}\n",
        "optimizer1 = optim.SGD([param_group])\n",
        "\n",
        "import copy\n",
        "clf1.aux = nn.Parameter(torch.zeros(args.train_len,args.num_classes),requires_grad = True) ##Creation of new params\n",
        "clf1.aux_lambda = nn.Parameter(torch.tensor([0.25]),requires_grad = False)\n",
        "init_model = copy.deepcopy(clf1)\n",
        "aux = {'params': [clf1.aux], 'lr' : 1, 'weight_decay': 0.001}\n",
        "optimizer1.add_param_group(aux)\n",
        "clf1.cuda()\n",
        "\n",
        "with open(txtfile, \"a\") as myfile:\n",
        "    myfile.write('epoch train_acc1 val_acc1 test_acc1\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjSDGKdI71l0"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYvrshxk6Ekm"
      },
      "source": [
        "## Model boilerplate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt2sq6rY6Etg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63a426b-0392-4e0b-d3db-793cdc7b3e25"
      },
      "source": [
        "epoch = 0\n",
        "train_acc1 = 0\n",
        "\n",
        "\n",
        "# evaluate models with random weights\n",
        "val_acc1 = evaluate(val_loader, clf1)\n",
        "print('Epoch [%d/%d] Val Accuracy on the %s val data: Model1 %.4f %%' % (\n",
        "epoch + 1, args.n_epoch, len(val_dataset), val_acc1))\n",
        "\n",
        "test_acc1 = evaluate(test_loader, clf1)\n",
        "print('Epoch [%d/%d] Test Accuracy on the %s test data: Model1 %.4f %%' % (\n",
        "epoch + 1, args.n_epoch, len(test_dataset), test_acc1))\n",
        "# save results\n",
        "with open(txtfile, \"a\") as myfile:\n",
        "    myfile.write(str(int(epoch)) + ' ' + str(train_acc1) + ' ' + str(val_acc1) + ' ' + str(test_acc1) + \"\\n\")\n",
        "val_acc_list = []\n",
        "test_acc_list = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] Val Accuracy on the 6000 val data: Model1 10.1500 %\n",
            "Epoch [1/100] Test Accuracy on the 10000 test data: Model1 10.8700 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DISWWOqc93kq"
      },
      "source": [
        "## Model training starts from here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZzfidzw93sa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "7075fc71-e751-45f4-fc83-0e72239c4a79"
      },
      "source": [
        "args.n_epoch = 40\n",
        "for epoch in range(0, args.n_epoch):\n",
        "    scheduler1.step()\n",
        "    clf1.train()\n",
        "    # print(optimizer1)\n",
        "    train_acc1 = train(train_loader, epoch, clf1, optimizer1, args)\n",
        "    val_acc1 = evaluate(val_loader, clf1)\n",
        "    val_acc_list.append(val_acc1)\n",
        "    test_acc1 = evaluate(test_loader, clf1)\n",
        "    test_acc_list.append(test_acc1)\n",
        "    \n",
        "    # save results\n",
        "    print('Epoch [%d/%d] Test Accuracy on the %s test data: Model1 %.4f %% ' % (\n",
        "    epoch + 1, args.n_epoch, len(test_dataset), test_acc1))\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write(str(int(epoch)) + ' ' + str(train_acc1) + ' ' + str(val_acc1) + ' ' + str(test_acc1) + \"\\n\")\n",
        "id = np.argmax(np.array(val_acc_list))\n",
        "test_acc_max = test_acc_list[id]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/40], Iter [350/1687] Training Accuracy1: 12.5000, Loss1: 2.4644\n",
            "Epoch [1/40], Iter [700/1687] Training Accuracy1: 6.2500, Loss1: 2.2611\n",
            "Epoch [1/40], Iter [1050/1687] Training Accuracy1: 28.1250, Loss1: 2.1438\n",
            "Epoch [1/40], Iter [1400/1687] Training Accuracy1: 18.7500, Loss1: 2.2344\n",
            "Epoch [1/40] Test Accuracy on the 10000 test data: Model1 70.8500 % \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-ec104aa0b9b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(optimizer1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_acc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_acc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-573bda578016>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, model1, optimizer1, args)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m### Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mprec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-ed2498103c0a>\u001b[0m in \u001b[0;36mtrain_one_step\u001b[0;34m(net, data, label, indexes, optimizer, criterion, nonzero_ratio, clip)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN9l0ToMOltz",
        "outputId": "6e1a3660-c94c-4ac2-9a59-fdef2f88e6a8"
      },
      "source": [
        "np.argmax((clf1.aux_lambda * clf1.aux).detach().cpu().numpy(),axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 1, 4, ..., 2, 5, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcnYeRGIimDt",
        "outputId": "b4fd2da8-ff19-44b5-ff64-665321a209f9"
      },
      "source": [
        "accuracy((clf1.aux_lambda * clf1.aux).detach().cpu(), torch.tensor(train_dataset.train_labels), topk=(1, ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([100.])]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxyzrv_a_ZOE"
      },
      "source": [
        "train_total = 0\n",
        "train_correct = 0\n",
        "train_correct_1 = 0\n",
        "for i, (data, labels, indexes) in enumerate(train_loader):\n",
        "    ind=indexes.cpu().numpy().transpose()\n",
        "    data = data.cuda()\n",
        "    labels = labels.cuda()\n",
        "    # Forward + Backward + Optimize\n",
        "    logits1=clf1(data)\n",
        "    prec1,  = accuracy(logits1, labels, topk=(1, ))\n",
        "    train_correct+=prec1\n",
        "    aux_output = clf1.aux_lambda * clf1.aux[indexes]\n",
        "    logits1 += aux_output\n",
        "    prec1,  = accuracy(logits1, labels, topk=(1, ))\n",
        "    train_total+=1\n",
        "    train_correct_1+=prec1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlbQ3QRqASIs",
        "outputId": "9c1f4ed0-c363-42ae-d11b-37a2428842c6"
      },
      "source": [
        "(train_correct/train_total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([29.4820], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd5qOB9jAXe3",
        "outputId": "a465d305-cec7-4629-802b-59ab8a728b64"
      },
      "source": [
        "(train_correct_1/train_total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([98.5819], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9htn6iycCjKn",
        "outputId": "fdc7ef80-8684-40ca-b656-9b3335a76242"
      },
      "source": [
        "test_acc_max"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.35"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0libLxyagiOm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}