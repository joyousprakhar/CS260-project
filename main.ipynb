{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.autograd import Variable\n", "import torchvision.transforms as transforms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from model import LeNet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from torch.optim.lr_scheduler import MultiStepLR\n", "import torch.backends.cudnn as cudnn\n", "import torchvision.models as tv_models\n", "import torch.optim as optim\n", "import argparse, sys\n", "import numpy as np\n", "import datetime\n", "import data_load\n", "import resnet\n", "import tools"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["warnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser = argparse.ArgumentParser()\n", "parser.add_argument('--n', type=int, default=0, help=\"No.\")\n", "parser.add_argument('--d', type=str, default='output', help=\"description\")\n", "parser.add_argument('--p', type=int, default=0, help=\"print\")\n", "parser.add_argument('--c', type=int, default=10, help=\"class\")\n", "parser.add_argument('--lr', type=float, default=0.01)\n", "parser.add_argument('--result_dir', type=str, help='dir to save result txt files', default='output/results_cdr/')\n", "parser.add_argument('--noise_rate', type=float, help='overall corruption rate, should be less than 1', default=0.4)\n", "parser.add_argument('--noise_type', type=str, help='[pairflip, symmetric, asymmetric]', default='symmetric')\n", "parser.add_argument('--num_gradual', type=int, default=10, help='how many epochs for linear drop rate')\n", "parser.add_argument('--dataset', type=str, help='mnist, fmnist, cifar10, cifar100', default='cifar10')\n", "parser.add_argument('--n_epoch', type=int, default=100)\n", "parser.add_argument('--optimizer', type=str, default='SGD')\n", "parser.add_argument('--seed', type=int, default=1)\n", "parser.add_argument('--print_freq', type=int, default=350)\n", "parser.add_argument('--num_workers', type=int, default=4, help='how many subprocesses to use for data loading')\n", "parser.add_argument('--model_type', type=str, help='[ce, ours]', default='cdr')\n", "parser.add_argument('--fr_type', type=str, help='forget rate type', default='type_1')\n", "parser.add_argument('--split_percentage', type=float, help='train and validation', default=0.9)\n", "parser.add_argument('--gpu', type=int, help='ind of gpu', default=0)\n", "parser.add_argument('--weight_decay', type=float, help='l2', default=1e-3)\n", "parser.add_argument('--momentum', type=int, help='momentum', default=0.9)\n", "parser.add_argument('--batch_size', type=int, help='batch_size', default=32)\n", "parser.add_argument('--train_len', type=int, help='the number of training data', default=54000)\n", "args = parser.parse_args()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(args)\n", "# Seed\n", "torch.manual_seed(args.seed)\n", "torch.cuda.manual_seed(args.seed)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Hyper Parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["learning_rate = args.lr"]}, {"cell_type": "markdown", "metadata": {}, "source": ["load dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(args):\n", "    np.random.seed(args.seed)\n", "    torch.manual_seed(args.seed)\n", "    torch.cuda.manual_seed(args.seed)\n", "    \n", "    if args.dataset=='fmnist':\n", "        args.channel = 1\n", "        args.feature_size = 28 * 28\n", "        args.num_classes = 10\n", "        args.n_epoch = 100\n", "        args.batch_size = 32\n", "        args.num_gradual = 10\n", "        args.train_len = int(60000 * 0.9)\n", "        train_dataset = data_load.fmnist_dataset(True,\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n", "                                        target_transform=tools.transform_target,\n", "                                        dataset=args.dataset,\n", "                                        noise_type=args.noise_type,\n", "                                        noise_rate=args.noise_rate,\n", "                                        split_per=args.split_percentage,\n", "                                        random_seed=args.seed)\n", "        val_dataset = data_load.fmnist_dataset(False,\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n", "                                        target_transform=tools.transform_target,\n", "                                        dataset=args.dataset,\n", "                                        noise_type=args.noise_type,\n", "                                        noise_rate=args.noise_rate,\n", "                                        split_per=args.split_percentage,\n", "                                        random_seed=args.seed)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        test_dataset = data_load.fmnist_test_dataset(\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n", "                                        target_transform=tools.transform_target)\n", "    \n", "    \n", "    if args.dataset=='mnist':\n", "        args.channel = 1\n", "        args.feature_size = 28 * 28\n", "        args.num_classes = 10\n", "        args.n_epoch = 100\n", "        args.batch_size = 32\n", "        args.num_gradual = 10\n", "        args.train_len = int(60000 * 0.9)\n", "        train_dataset = data_load.mnist_dataset(True,\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n", "                                        target_transform=tools.transform_target,\n", "                                        dataset=args.dataset,\n", "                                        noise_type=args.noise_type,\n", "                                        noise_rate=args.noise_rate,\n", "                                        split_per=args.split_percentage,\n", "                                        random_seed=args.seed)\n", "        val_dataset = data_load.mnist_dataset(False,\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n", "                                        target_transform=tools.transform_target,\n", "                                        dataset=args.dataset,\n", "                                        noise_type=args.noise_type,\n", "                                        noise_rate=args.noise_rate,\n", "                                        split_per=args.split_percentage,\n", "                                        random_seed=args.seed)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        test_dataset = data_load.mnist_test_dataset(\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.1307, ),(0.3081, )),]),\n", "                                        target_transform=tools.transform_target)\n", "        \n", "        \n", "    \n", "    if args.dataset=='cifar10':\n", "        args.channel = 3\n", "        args.num_classes = 10\n", "        args.feature_size = 3 * 32 * 32\n", "        args.n_epoch = 200\n", "        args.batch_size = 64\n", "        args.num_gradual = 20\n", "        args.train_len = int(50000 * 0.9)\n", "        train_dataset = data_load.cifar10_dataset(True,\n", "                                        transform = transforms.Compose([\n", "                                        transforms.RandomCrop(32, padding=4),\n", "                                        transforms.RandomHorizontalFlip(),\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n", "                                        ]),\n", "                                        target_transform=tools.transform_target,\n", "                                        dataset=args.dataset,\n", "                                        noise_type=args.noise_type,\n", "                                        noise_rate=args.noise_rate,\n", "                                        split_per=args.split_percentage,\n", "                                        random_seed=args.seed)\n", "        val_dataset = data_load.cifar10_dataset(False,\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n", "                                        ]),\n", "                                        target_transform=tools.transform_target,\n", "                                        dataset=args.dataset,\n", "                                        noise_type=args.noise_type,\n", "                                        noise_rate=args.noise_rate,\n", "                                        split_per=args.split_percentage,\n", "                                        random_seed=args.seed)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        test_dataset = data_load.cifar10_test_dataset(\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n", "                                        ]),\n", "                                        target_transform=tools.transform_target)\n", "    \n", "    \n", "    if args.dataset=='cifar100':\n", "        args.channel = 3\n", "        args.num_classes = 100\n", "        args.feature_size = 3 * 32 * 32\n", "        args.n_epoch = 200\n", "        args.batch_size = 64\n", "        args.num_gradual = 20\n", "        args.train_len = int(50000 * 0.9)\n", "        train_dataset = data_load.cifar100_dataset(True,\n", "                                        transform = transforms.Compose([\n", "                                        transforms.RandomCrop(32, padding=4),\n", "                                        transforms.RandomHorizontalFlip(),\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n", "                                        ]),\n", "                                        target_transform=tools.transform_target,\n", "                                        dataset=args.dataset,\n", "                                        noise_type=args.noise_type,\n", "                                        noise_rate=args.noise_rate,\n", "                                        split_per=args.split_percentage,\n", "                                        random_seed=args.seed)\n", "        val_dataset = data_load.cifar100_dataset(False,\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n", "                                        ]),\n", "                                        target_transform=tools.transform_target,\n", "                                        dataset=args.dataset,\n", "                                        noise_type=args.noise_type,\n", "                                        noise_rate=args.noise_rate,\n", "                                        split_per=args.split_percentage,\n", "                                        random_seed=args.seed)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        test_dataset = data_load.cifar100_test_dataset(\n", "                                        transform = transforms.Compose([\n", "                                        transforms.ToTensor(),\n", "                                        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n", "                                        ]),\n", "                                        target_transform=tools.transform_target)\n", "        \n", "    \n", "    return train_dataset, val_dataset, test_dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["save_dir = args.result_dir + '/' + args.dataset + '/%s/' % args.model_type"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not os.path.exists(save_dir):\n", "    os.system('mkdir -p %s' % save_dir)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def accuracy(logit, target, topk=(1,)):\n", "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n", "    output = F.softmax(logit, dim=1)\n", "    maxk = max(topk)\n", "    batch_size = target.size(0)\n", "    _, pred = output.topk(maxk, 1, True, True)\n", "    pred = pred.t()\n", "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n", "    res = []\n", "    for k in topk:\n", "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n", "        res.append(correct_k.mul_(100.0 / batch_size))\n", "    return res"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_one_step(net, data, label, optimizer, criterion, nonzero_ratio, clip):\n", "    net.train()\n", "    pred = net(data)\n", "    loss = criterion(pred, label)\n", "    loss.backward()\n", "    \n", "    to_concat_g = []\n", "    to_concat_v = []\n", "    for name, param in net.named_parameters():\n", "        if param.dim() in [2, 4]:\n", "            to_concat_g.append(param.grad.data.view(-1))\n", "            to_concat_v.append(param.data.view(-1))\n", "    all_g = torch.cat(to_concat_g)\n", "    all_v = torch.cat(to_concat_v)\n", "    metric = torch.abs(all_g * all_v)\n", "    num_params = all_v.size(0)\n", "    nz = int(nonzero_ratio * num_params)\n", "    top_values, _ = torch.topk(metric, nz)\n", "    thresh = top_values[-1]\n", "    for name, param in net.named_parameters():\n", "        if param.dim() in [2, 4]:\n", "            mask = (torch.abs(param.data * param.grad.data) >= thresh).type(torch.cuda.FloatTensor)\n", "            mask = mask * clip\n", "            param.grad.data = mask * param.grad.data\n", "    optimizer.step()\n", "    optimizer.zero_grad()\n", "    acc = accuracy(pred, label, topk=(1,))\n", "    return float(acc[0]), loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(train_loader, epoch, model1, optimizer1, args):\n", "    model1.train()\n", "    train_total=0\n", "    train_correct=0\n", "    clip_narry = np.linspace(1-args.noise_rate, 1, num=args.num_gradual)\n", "    clip_narry = clip_narry[::-1]\n", "    if epoch < args.num_gradual:\n", "        clip = clip_narry[epoch]\n", "   \n", "    clip = (1 - args.noise_rate)\n", "    for i, (data, labels, indexes) in enumerate(train_loader):\n", "        ind=indexes.cpu().numpy().transpose()\n", "        data = data.cuda()\n", "        labels = labels.cuda()\n", "        # Forward + Backward + Optimize\n", "        logits1=model1(data)\n", "        prec1,  = accuracy(logits1, labels, topk=(1, ))\n", "        train_total+=1\n", "        train_correct+=prec1\n", "        # Loss transfer \n", "        prec1, loss = train_one_step(model1, data, labels, optimizer1, nn.CrossEntropyLoss(), clip, clip)\n", "       \n", "        if (i+1) % args.print_freq == 0:\n", "            print('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Loss1: %.4f' \n", "                  %(epoch+1, args.n_epoch, i+1, args.train_len//args.batch_size, prec1, loss.item()))\n", "        \n", "      \n", "    train_acc1=float(train_correct)/float(train_total)\n", "    return train_acc1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate the Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate(test_loader, model1):\n", "    \n", "    model1.eval()  # Change model to 'eval' mode.\n", "    correct1 = 0\n", "    total1 = 0\n", "    with torch.no_grad():\n", "        for data, labels, _ in test_loader:\n", "            data = data.cuda()\n", "            logits1 = model1(data)\n", "            outputs1 = F.softmax(logits1, dim=1)\n", "            _, pred1 = torch.max(outputs1.data, 1)\n", "            total1 += labels.size(0)\n", "            correct1 += (pred1.cpu() == labels.long()).sum()\n", "        acc1 = 100 * float(correct1) / float(total1)\n", "    return acc1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(args):\n", "    # Data Loader (Input Pipeline)\n", "    model_str = args.dataset + '_%s_' % args.model_type + args.noise_type + '_' + str(args.noise_rate) + '_' + str(args.seed)\n", "    txtfile = save_dir + \"/\" + model_str + \".txt\"\n", "    nowTime = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n", "    if os.path.exists(txtfile):\n", "        os.system('mv %s %s' % (txtfile, txtfile + \".bak-%s\" % nowTime))\n", "    \n", "    # Data Loader (Input Pipeline)\n", "    print('loading dataset...')\n", "    train_dataset, val_dataset, test_dataset = load_data(args)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n", "                                               batch_size=args.batch_size,\n", "                                               num_workers=args.num_workers,\n", "                                               drop_last=False,\n", "                                               shuffle=True)\n", "    \n", "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n", "                                              batch_size=args.batch_size,\n", "                                              num_workers=args.num_workers,\n", "                                              drop_last=False,\n", "                                              shuffle=False)\n", "    \n", "    \n", "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n", "                                              batch_size=args.batch_size,\n", "                                              num_workers=args.num_workers,\n", "                                              drop_last=False,\n", "                                              shuffle=False)\n", "    \n", "    \n", "    \n", "    # Define models\n", "    print('building model...')\n", "    \n", "    if args.dataset == 'mnist':\n", "        clf1 = LeNet()\n", "        optimizer1 = torch.optim.SGD(clf1.parameters(), lr=learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n", "        scheduler1 = MultiStepLR(optimizer1, milestones=[10, 20], gamma=0.1)\n", "    elif args.dataset == 'fmnist':\n", "        clf1 = resnet.ResNet50(input_channel=1, num_classes=10)\n", "        optimizer1 = torch.optim.SGD(clf1.parameters(), lr=learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n", "        scheduler1 = MultiStepLR(optimizer1, milestones=[10, 20], gamma=0.1)\n", "    elif args.dataset == 'cifar10':\n", "        clf1 = resnet.ResNet50(input_channel=3, num_classes=10)\n", "        optimizer1 = torch.optim.SGD(clf1.parameters(), lr=learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n", "        scheduler1 = MultiStepLR(optimizer1, milestones=[40, 80], gamma=0.1)\n", "    elif args.dataset == 'cifar100':\n", "        clf1 = resnet.ResNet50(input_channel=3, num_classes=100)\n", "        optimizer1 = torch.optim.SGD(clf1.parameters(), lr=learning_rate, weight_decay=args.weight_decay, momentum=0.9)\n", "        scheduler1 = MultiStepLR(optimizer1, milestones=[40, 80], gamma=0.1)\n", "        \n", "    clf1.cuda()\n", "    \n", "    with open(txtfile, \"a\") as myfile:\n", "        myfile.write('epoch train_acc1 val_acc1 test_acc1\\n')\n", "    epoch = 0\n", "    train_acc1 = 0\n", "   \n", "    \n", "    # evaluate models with random weights\n", "    val_acc1 = evaluate(val_loader, clf1)\n", "    print('Epoch [%d/%d] Val Accuracy on the %s val data: Model1 %.4f %%' % (\n", "    epoch + 1, args.n_epoch, len(val_dataset), val_acc1))\n", "    \n", "    test_acc1 = evaluate(test_loader, clf1)\n", "    print('Epoch [%d/%d] Test Accuracy on the %s test data: Model1 %.4f %%' % (\n", "    epoch + 1, args.n_epoch, len(test_dataset), test_acc1))\n", "    # save results\n", "    with open(txtfile, \"a\") as myfile:\n", "        myfile.write(str(int(epoch)) + ' ' + str(train_acc1) + ' ' + str(val_acc1) + ' ' + str(test_acc1) + \"\\n\")\n", "    val_acc_list = []\n", "    test_acc_list = []  \n", "    \n", "    for epoch in range(0, args.n_epoch):\n", "        scheduler1.step()\n", "        print(optimizer1.state_dict()['param_groups'][0]['lr'])\n", "        clf1.train()\n", "        \n", "        train_acc1 = train(train_loader, epoch, clf1, optimizer1, args)\n", "        val_acc1 = evaluate(val_loader, clf1)\n", "        val_acc_list.append(val_acc1)\n", "        test_acc1 = evaluate(test_loader, clf1)\n", "        test_acc_list.append(test_acc1)\n", "        \n", "        # save results\n", "        print('Epoch [%d/%d] Test Accuracy on the %s test data: Model1 %.4f %% ' % (\n", "        epoch + 1, args.n_epoch, len(test_dataset), test_acc1))\n", "        with open(txtfile, \"a\") as myfile:\n", "            myfile.write(str(int(epoch)) + ' ' + str(train_acc1) + ' ' + str(val_acc1) + ' ' + str(test_acc1) + \"\\n\")\n", "    id = np.argmax(np.array(val_acc_list))\n", "    test_acc_max = test_acc_list[id]\n", "    return test_acc_max"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    best_acc = main(args)\n", "    "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}